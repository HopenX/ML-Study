# 机器学习概述
## 机器学习分类
1. 监督学习：已经有数据，和数据对应的标签。
2. 非监督学习：给定的样本无需输出/标签，让机器自己学习样本中隐含的内部结构。
3. 半监督学习：二者结合。
4. 强化学习：通过打分/评价的形式，类似于监督学习中的标签。

## 机器学习模型
机器学习 = 数据 data + 模型 model + 优化方法 optimal strategy

## 偏差/方差权衡
来自 Wikipedia：
> 在监督学习中，如果能将模型的方差与误差权衡好，那么可以认为该模型的泛化性能（对于新数据）将会表现出好的结果。

>偏差刻画的是算法本身的性能。高偏差将会造成欠拟合(Underfitting) [miss the relevant relations between features and target outputs]。换句话说，模型越复杂偏差就越小；而模型越简单，偏差就越大。

>方差用来衡量因训练集数据波动(fluctuations)而造成的误差影响。高方差将会造成过拟合(Overfitting)。

在周志华老师<机器学习>书中是这样阐述的：

>*偏差* 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；

>*方差* 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；

>*噪声* 则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题的本身难度

>偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定的学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使数据扰动产生的影响小。一般来说方差与偏差是有冲突的，这称为方差-偏差窘境。

## 常见机器学习算法概览
### 1.Linear Algorithm 线性算法
1. **Linear Regression 线性回归**：使用最小二乘法 Least Squares 拟合一条直线 → 计算 R<sup>2</sup> → 计算 R<sup>2</sup> 的 p 值。R<sup>2</sup> 表示 x 能多大程度反映 y 的变化，p 值表示可靠程度。拟合直线的过程使用「随机梯度下降」（SGD）

2. **Lasso 回归 和 Ridge 回归**：都可以减少共线性带来的影响，即 X 自变量之间有相互关联。区别可以归结为L2和L1正则化的性质差异。

3. **Polynomial Regression 多项式回归**：能够模拟非线性可分的数据（曲线），线性回归不能做到这一点。但容易过拟合。

4. **Logistic Regression 逻辑回归**：判断 True or False，Y 值为 0-1 表示概率，用于分类。线性回归使用「Residual 偏差」，而逻辑回归使用「maximum likelihood 最大似然」

### 2.Decision Tree 决策树
1. **ID3**: 计算「信息熵」 Ent(D)，值越小，说明样本集合D的纯度就越高，进而选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的「信息增益」 Gain(D, a)，值越大，说明如果用属性a来划分样本集合D，那么纯度会提升。

2. **C4.5**: 提出Gainratio 「增益率」，解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱。

3. **CART(Classification and Regression Tree)**: 通过计算 Gini 基尼系数（尽可能小），判断 impurity 不纯洁度。离散数据用「是否」划分子树，连续数据可以用「两两之间平均值」划分子树。

### 3.SVM 支持向量机
SVM：https://blog.csdn.net/liugan528/article/details/79448379
KKT：https://blog.csdn.net/qq_32763149/article/details/81055062

SVM 分类：

1. 硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。
2. 软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化得到一个线性支持向量机。
3. 非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化得一个非线性支持向量机。

基本原理：

1. Maximum Margin Classifier：只看边界。

2. Soft Margin Classifier（即 Support Vector Classifier）：允许 misclassification，寻找两个支撑向量来确定分类边界。

3. Kernel Function：从低维数据开始，通过「核函数」给数据升维，然后找到一个 Support Vector Classifier 将数据分成两组。核函数的选择，支撑向量的选择，都用 cross validation 交叉验证。Radial Basis Function（RBF）Kernel 基于邻近。

4. Kernel Trick: 根据升维的距离进行计算，但是不进行实际的升维

部分术语：
1. 拉格朗日对偶性：解决「凸二次规划」（convex quadratic propgramming）问题

2. KKT 条件：是拉格朗日乘子的泛化，把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + a*g(x) + b*h(x)，KKT条件是说最优值必须满足以下条件：（1）L(a, b, x)对x求导为零；（2）h(x) =0; （3）a*g(x) = 0;

3. SMO：Sequential Minimal Optimization用二次规划来求解α，要用到 KKT

4. SVR：支持向量回归

优点：
SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。

缺点：
SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)O(n2)的复杂度，这对大规模的数据是吃不消的。

### 4.Naive Bayes Algorithms 朴素贝叶斯
